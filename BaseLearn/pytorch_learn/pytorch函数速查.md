torch.range(start=1, end=6)的结果是会包含end的
而torch.arange(start=1, end=6)的结果并不包含end
两者创建的tensor的类型也不一样,前者float32，后者int64

torch.view()

在pytorch中view函数的作用为重构张量的维度尺寸，相当于numpy中resize（）的功能，但是用法可能不太一样。如下例所示
torch.view(可变长参数表示各个维度重构后的大小)
维度大小为-1 表示该维度自动计算
torch.view(-1) 比如，变成一维, 长度自动计算
torch.view(2, 3, -1, 4, 5 ) 第三个维度自动计算

permute()
permute译为重新排列，高维转置操作，从代数上理解就是加入就是通过permute(..j...)，将第j个维度放到第i个维度上，
那么下标访问的时候，新的tentor访问第i个维度xx下标，就相当于访问原来第j个维度的xx下标，
举例子a.permute(1,2,0)，那么新的a[3,4,5]=原来的a[4,5,3], 即第0维度与第1个维度交换，现在第0个维度下标3，就相当于访问原来的第1维下标3
注意这个 从几何上不好理解的
区别
和transpose的区别， transpose只能用于2维，意思转置，但两者本质上有共同性
和View区别，permute和view都有改变尺寸的作用，但view是只改变尺寸，不考虑下标的重新排列关系，它是把整个数据拉平，然后重新改变尺寸，它能变成各种尺寸，但permute不行，只能重新排列尺寸
应用上对于一个图像batch，其形状为[batch, channel, height, width]，我们可以使用tensor.permute(0,3,2,1)得到形状为[batch, width, height, channel]的tensor.

torch.repeat(维度，可变长参数)
1、注意这个函数的形式，直接传入每个维度内部重复次数，不是指定维度，再指定重复次数，这种糟糕的方式
2、注意是重复维度内部，不是整个维度，比如tenseor([1,2]).repeat(2), 重复第0维内部=[1,2,1,2],不是整个第0维，变成[[1,2],[1,2]]
3、传入的维度数量可以比原始维度数量更多，比如2维的传入3个维度参数，然后重复规则是先将当前tensor扩充到3维，然后再进行重复, 传入更少的重复应该也没必要不支持吧
例子
>> x = torch.tensor([1, 2, 3])
>> x.repeat(3, 2)
tensor([[1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3],
        [1, 2, 3, 1, 2, 3]])

下标操作相关

torch的tensor切片的时候还能增加维度，通过切片操作在增加的维度上面传入None的方式
二维的a这样写变成3维
a[:, :, None]
还可以在下标中括号运算里面传入列表[a,b,c]，表示取第0维的a,b,c号元素，组成列表，还可以是多维的！秀！


torch.meshgrid(x, y)
平面上画网格的函数，就是传上x轴上的点，y轴上的点，产生len(x) * len(y)个点
返回值是两个二维矩阵的形式，分别表示每个点的x和y坐标
这个函数还真没啥意思吧，自己写就行了，不然还得去理解api

save和load
save的可以是任意对象，它只是特别的对张量做了处理，其它和python的一张
load的时候可以指定map_location参数
        默认情况下，参数先读取为cpu形式，再转移到保存时的形式，比如gpu，
         map_location参数指定要转移到哪种形式，它可以是device 的形式，还可以是函数的形式，怎么用未知
         
## squeeze() 
torch.squeeze(input, dim=None, out=None) → Tensor
官方文档：https://pytorch.org/docs/master/generated/torch.squeeze.html
参数说明：
input (Tensor)：输入的张量
dim (int, optional) ：可选参数，如果不指定，该方法会把所有值为 1 的维度移除，如果指定，该方法则指移除指定的那个维度
out (Tensor, optional) ：可选，指定输出的张量.

## torch.cat(list, dim)
cat是concatnate的意思：拼接 
在给定维度上对输入的张量序列seq 进行连接操作
对于不同的参数形式，要风情拼接的是哪个，通过边长参数输入，拼接参数，通过list输入，拼接list内部的数据，不是list
参数
inputs : 待连接的张量序列，可以是任意相同Tensor类型的python 序列
dim : 选择的扩维, 必须在0到len(inputs[0])之间，沿着此维连接张量序列。
2. 重点
输入数据必须是序列，序列中数据是任意相同的shape的同类型tensor

## module及其子类以及模型 层之间的关系 网址
https://zhuanlan.zhihu.com/p/282863934 
sequential 放入的大概也分为两类，就是列表和字典map
https://zhuanlan.zhihu.com/p/340453841

分布式并行：
使用上只需要调用model = nn.DataParallel(model),然后就像一个单卡的模型一样使用即可，绝大部分情况下对用户来说并行化是透明的，逻辑交给框架内部来完成，还是很厉害的   
要注意的几个点是使用分布式的时候学习率要调整（损失函数等等要根据情况，比如批数量增大等，看是否调整）

基本原理，首先要明白非并行情况下前向传播和反向传播的流程，画个计算图可以知道，批量梯度下降前向传播中每个样本都会用到参数w...，所以这个参数有多条(m+条)路径链接到最后的输出中，反向传播的时候需要累加偏导数

基于这个，分布式就是类似的，把若干个样本对应的路径合成一组，然后累加梯度，最后将所有的组的梯度再累加，就和原来一样了，简单的想就是计算图上把若干条路径归到一组

具体实现流程就是，用一个管理者GPU，把当前参数模型传递给不同的GPU，然后将样本分成多个组，交给不同的GPU，每个GPU进行前向传播，然后反向传播求出偏导，最后管理GPU整合所以GPU的偏导数
注意这里使用了装饰器模式，也是这个模式的优点，透明的，用户可以像使用原来的接口一样使用

然后重写了forward函数，在里面进行样本分配和参数整合
 scatter 函数，负责将 tensor 分成大概相等的块并将他们分给不同的 GPU。对其他的数据类型，则是复制给不同的 GPU 。