# 代码项目结构
experiments 目录下是存放的各种网络结构的配置,通过yaml文件编写
external里面放的是借助的外部库，这些库提供目标跟踪的辅助功能工具，比如比如作者提到的pytracking 库，就是专门的python tacking辅助工具 还有数据集vot 自己提供的数据集解析的库
## lib目录
lib目录下也有一个config，里面是各种型号跟踪器的配置
然后是models 其下的stark目录，是stark各种不同型号网络用到的模型组件，比如什么backbone transformer之类的

# 运行流程
## 测试流程
tracking目录下的test文件main进入测试流程

加载数据，lasot的数据是视频帧和帧相关数据的文本，很容易读取  

创建模型，注意本文测试时的跟踪器tracker包裹了三层，第一层是跟踪辅助类，![lib/test/evaluation/tracker](lib/test/evaluation/tracker.py)，它包含一些运行需要的辅助信息，比如配置获取和设置什么的，还有帧序列的循环，第二层是测试包装类，包括lib/test/tracker 下的不同类型的跟踪模型,比如![stark_st](lib/test/tracker/stark_st.py)，这是为测试而打包的包装了，包装了测试需要的一些东西，比如no_grade模式，第三层才是最终的跟踪器模型.比如![stark_st](lib/models/stark/stark_st.py)， 

创建模型的流程是在test里面先创建第一层的跟踪器辅助类，然后调用到run_dataset进行运行流程, 经过一系列跳转，最终到了test/evaluation中的tracker文件中的 create_tracker 方法里面才创建的模型,   
create_tracker是使用的类对象创建跟踪器对象，这个类对象是前面的流程里面根据配置用python 的import_module创建的，这里创建了跟踪器的第二层测试包装类，测试包装类再初始化方法里面创建追踪的跟踪器对象，跟踪器内部是通过build_xxx方法构建跟踪器网络的不同模块，比如主干，transformer部分，预测头部分，把它们作为跟踪器的属性，然后就得到了跟踪器类，然后在运行的时候，forward不同的部分即可

运行模型，运行最终走到的主方法就是上一步的![lib/test/evaluation/tracker](lib/test/evaluation/tracker.py)类中的 _track_sequence 方法，这个方法内部会先根据调用跟踪器的初始化方法，跟踪器就是上面提到的第二层的跟踪器![stark_st](lib/test/tracker/stark_st.py)，用第一帧和相关数据初始化跟踪器，主要就是经过backbone获取跟踪对象的特征   
然后就是在后续帧进行跟踪，后续帧首先经过主干网络，然后输出特征和目标特征拼接，然后经过transformer提取特征，
然后经过预测头和打分网络，注意这两个作者都在 forward_transformer 里面一起运行了，最后处理结果就完事儿了


# 模块化
**模块化，且模块支持增删改换**
这个模型结构也是模块化的，模块间具有可置换性，可以对不同的部分配置不同的模块，这和一般软件工程是一样的
比如它的主干模块可以采用不同的网络主干
时间信息模块可以加入也可以不加入


一般的数据集，是把视频处理好分成帧了的
对于每个视频，可以构造一个序列对象，序列对象的元素是以视频帧为中心的各种信息，包括帧路径，对象位置，对象是否完全看不见等信息，然后还包含整个视频的一些信息

设置一个模型的装饰器，用来处理模型运行配置等工作


# transformer相关
这篇文章的encoder直接堆叠完全相同的self-attention+ffn 形成编码器
自注意力层很好添加，直接用pytorch提供的类即可，根cnn一样的
然后q k v 是要自己计算然后输入，不是自动计算的
文中提到的解码器查询输入，直接就是用的某种嵌入网络的参数

sa的长度是不限定的

# 轻量化相关
作者后来显然收到swin-transformer的启发，整了一个swin版本的transformer
  
使用repvgg作为轻量化模型的主干，这个主干主要是速度快，详见 
https://blog.csdn.net/weixin_48249563/article/details/115030808

## 轻量化transformer
输入序列向量长度128,标准版本的是256，多头数量为8不变  
后面接的FFN数量不变  
似乎只有一个编码器，但是这个感觉不科学，transformer都是具有两个解码器的呀 为什么?
编码器只一个注意力模块，而不是标准版本的堆叠6个模块   
使用了新的位置编码方式   
最后的边框预测网络使用的也是repvgg的block，包含5个卷积，其中2个是1*1 size的

作者的回答：
@ juicelementlemon Hi，我们发现缺省公差(相对1e-3，绝对1e-5)可能不适合我们的网络输出。具体来说，测试结果(是否能够通过测试)也取决于输入。此外，我们发现 PyTorch 模型和 ONNX 模型的跟踪指标几乎相同。所以请随意使用提供的转换脚本。

# 关于对提出的目标和搜索区域使用掩膜的原因
https://github.com/researchmm/Stark/issues/53
作者在github上面的回答：
嗨，面具是用来处理填充物的。我们不希望网络(Transformer)学习填充区域的相似性，因为这些区域没有有价值的信息。掩模操作对跟踪性能没有明显影响。


# 延伸和改进
https://github.com/researchmm/Stark/issues/53 

lightning model does not have score head. Is this because that the performance is bad?
闪电型号没有得分头，是因为表现不好吗？
I find the frozen part. 我找到了冰冻的部分
Without the score, if the target is out of FOV (i.e. out of image), the lightning model will always product a wrong output coordinate.
如果没有得分，如果目标在视场之外(即图像之外) ，闪电模型总是产生一个错误的输出坐标。
Why do "sample target" on the complete image? I save the image, and find so many black padding region.
为什么要在完整的图像上做“样本目标”呢? 我保存了图像，发现有这么多的黑色填充区域。

## 2、@codylcs Hi, the sudden drift is partially caused by the lack of temporal-smoothness post-processing (such as cosine window, etc). We also find that STARK performs well in dealing with target disappearance and reappearance (especially important for long-term tracking), while struggles dealing similar distractors. So I think it could be a promising direction to improve the ability of STARK to deal with distractors.

@ codylcs 嗨，突然的漂移部分是由于缺乏时间平滑后处理(如余弦窗口等)造成的。我们还发现，斯塔克在处理目标消失和再现(特别重要的长期跟踪)方面表现良好，而处理类似的干扰。因此，我认为这可能是一个有希望的方向，以提高斯塔克处理干扰物的能力。

# 技巧学习
为测试专门写一个原始模型的装饰类？
目标检测测试的时候还是要跑在视频上才最能感受的模型效果

# 小细节：
作者提到没有必要使用 lmdb 格式