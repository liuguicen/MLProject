# Abstract
In this paper,  we present a new tracking architecture with an encoder-decoder transformer as the key compo-nent.  The encoder models the global spatio-temporal fea-ture dependencies between target objects and search re-gions, while the decoder learns a query embedding to pre-dict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem,  without using any proposals or predefined an-chors.  With the encoder-decoder transformer, the predic-tion of objects just uses a simple fully-convolutional net-work, which estimates the corners of objects directly.  The whole method is end-to-end, does not need any postprocess-ing steps such as cosine window and bounding box smooth-ing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on multiple challenging short-term and long-term benchmarks, while  running at  real-time speed,  being  6Ã— faster than Siam R-CNN [54].  Code and models are open-sourced at https://github.com/researchmm/Stark.
