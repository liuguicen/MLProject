# 第二课：
前面讲的，现在按照我们学深度学习的路径，傅里叶变换也基本不用学了

反向传播算法 梯度下降
已经很成熟了，我们去改的可能性为0，需要数学专业博士级别的

### 全连接nn：

有理论证明2层隐藏层已经够了，能逼近任何有理函数
中间的节点数 没有理论，经验性的理论 输入的1.2倍
神经网络的问题，样本量需求大
层数不能太深，太深会出现梯度消失，过拟合
不可解释性
以上的缺点导致了第二次神经网络热潮的褪去

### 支持向量机：
机 指的是算法
svm非线性变换 是选择非线性映射 即核函数 将非线性变换映射到高维空间变成线性的
比如使用极坐标变换分隔小圆和大圆
SVM优点很多，具有可解释性
数学上很复杂
我们的话需要能用的出来

### 深度学习：
深度是相对于前面的算法，前面的是浅层学习，只能学到比较浅的特征，或者说比较少的参数
深度能学到更深层次的特征，像图像这种具有上万参数的就能学习了

## 卷积NN：

最后加了全连接层，因为全连接的推理能力是最强的，前面的卷积部分更多作用是提取特征
最后全连接一般都只有2-3层，为什么，因为前面说的3层就够了
傅里叶变换 可以提取特征 小波变换 是它的进化版 也可以提取特征 现在也淘汰了

卷积原理可以和滤波器关联起来：
以前的算法，就是通过滤波器来提取特征，一个滤波器提取一种特征，比如边缘，中间值最大的3*3 矩阵，模糊 全为1/9的3*3矩阵
因此CNN中的卷积可以提取特征。
所以一个卷积核用于上一层的所有元素，因为是在图像的不同位置，用同一个卷积核提取同一种特征

自己按照那个书的方式学习 还是相当不科学的 学到的反而少
现在倾向于小卷积 3*3
卷积网络中各个数量的计算 一定要去推一下

自己搭卷积 这些细节
神经网络的数学理论比较简单 代码上的细节反而多 在AI方向上，通过NN，一般性非理论性工作上，学计算机的能比学数学的做得好，这在传统上是不这样的 
因为传统算法的公式推导 更多
因为NN的特性，写代码 搭网络训练的 过程 用了更多的时间 因为其不可解释性等，喝

### 池化：
一般会采用max pooling，这样就是把卷积学到的最大的特征提取出来 
池化作用：
减少不必要的特征，并且进行特征降维 一定程度上防止过拟合，有一定的正则化作用 
让下一层的卷积处理的视野更大，
减少参数，降低卷积的计算量
池化层能带来平移，旋转不变形 （就是说同一个图像旋转、平移了，传入CNN一样能得出同样的结果）
仔细想想，池化次是不是必要的了，似乎增大步长能替代池化层的很多作用
ICLR2015的一篇论文STRIVING FOR SIMPLICITY:THE ALL CoNVOLUTIONAL NET做了这方面的工作，作者在 CIFAR10/100 数据集上设计了一系列对比实验，
从而得出可以在不损失精度下，将最大池化层替换成卷积层。


sigmod 的问题
1、梯度消失
2、计算复杂度大， 需要指数函数，tanh 也是计算量大00
sigmod 理论上很完备

相对于的
relu函数：
正方向上不会梯度消失
求导快

relu函数有很多的变种，比如prelu 让负数区不消失

softmax 在机器学习和深度学习中有广泛的应用
相当于一个整理层
义如其名，最大的输入得到的输出最接近于1，包括负数

损失函数

卷积学到的特征图 可以看成很多图像形成的高维特征图，也相当于一张图
预训练的参数组成了这张图
从这个角度看，我们可以把它当成一张输入的图，也就是使用预训练网络

学习深度学习看代码是十分重要的-甚至最重要？

第三课：
一定去复现fastRCNN
首先通过一定方法框出候选的窗口-比如颜色聚类
非极大抑制法，去掉框，就是

框回归
利用回归预测的思想？
fast-RCNN放弃了分治合并的方法，里面有很多逻辑判断
采用并行式的方法，直接利用GPU等计算 快

做数据集，搭建网络

考虑一个问题，最后一层或者说第l层的CNN神经元，感受野大小




















